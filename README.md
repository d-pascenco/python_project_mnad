# Анализ репозиториев GitHub по AI/ML

Проект посвящён сбору и исследованию популярных GitHub-репозиториев с тегами **AI** и **ML**. Мы парсим результаты глобального поиска GitHub, объединяем данные в единый датасет, создаём новые признаки, проводим EDA и визуализации, чтобы выявить закономерности популярности open-source проектов.

## Источник данных
- Глобальный поиск по тегу AI: <https://github.com/search?q=AI&type=repositories&s=stars&o=desc>
- Глобальный поиск по тегу ML: <https://github.com/search?q=ML&type=repositories&s=stars&o=desc>

Каждый репозиторий собирается как отдельная строка с расширенным набором полей (звёзды, форки, язык, даты обновления и др.), что обеспечивает достаточное число наблюдений и признаков для анализа.

## Цель исследования
Собрать и очистить датасет популярных AI/ML-репозиториев, создать новые признаки и провести анализ, который помогает понять факторы востребованности проектов (звёзды, активность, язык и т.д.).

## Минимальные требования к датасету
- Количество наблюдений: > 1000 (для команды из 4 человек).
- Признаки: стартовый набор 8–10+ признаков разных типов.
- Новые признаки: минимум 2 дополнительно созданных.
- Визуализации: минимум 3 разных типов в итоговом ноутбуке.

## Используемые технологии
- Python 3.12.12
- pandas, NumPy
- requests, BeautifulSoup (bs4)
- time, random, itertools, pathlib, types.NoneType

## Команда и роли
- **Трофимов Матвей Владимирович** — сбор, парсинг, первичные выводы.
- **Самунджян Дина Арменаковна** — очистка данных, пропуски, дубликаты.
- **Серенко Елена Валерьевна** — создание новых признаков.
- **Пащенко Дмитрий Игоревич** — визуализации, выводы и результаты.

## Структура проекта
- `raw_parsed_data/` — сырые выгрузки (CSV) после парсинга GitHub.
- `raw_concat_dataset/` — объединённые наборы данных перед очисткой.
- `new_index_dataset/` — датасеты с добавленными индексами/признаками после обработки.
- `Проект по Python для анализа данных.ipynb` — основной ноутбук с кодом, промежуточными выводами и визуализациями.

## Рабочий процесс
1. Спарсить результаты поиска GitHub и сохранить CSV в `raw_parsed_data/`.
2. Объединить выгрузки в единый датасет (`raw_concat_dataset/`).
3. Очистить данные, обработать пропуски и выбросы, добавить новые признаки и индексы (`new_index_dataset/`).
4. Провести EDA и визуализации в ноутбуке, фиксируя промежуточные выводы в Markdown.

