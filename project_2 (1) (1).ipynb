{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# **Анализ репозиториев в GitHub**\n",
    "\n",
    "## 1. Введение\n",
    "### 1.1 Цель исследования\n",
    "Собрать данные о популярных GitHub-репозиториях по тегам AI и ML с помощью веб-парсинга результатов глобального поиска GitHub, провести очистку данных, создание новых признаков и исследовательский анализ, чтобы выявить закономерности популярности open-source проектов.\n",
    "\n",
    "### 1.2 Источник данных\n",
    "\n",
    "Мы выполняем парсинг HTML-страниц глобального поиска GitHub:\n",
    "\n",
    "Глобальный поиск по тегу AI:\n",
    "https://github.com/search?q=AI&type=repositories&s=stars&o=desc\n",
    "\n",
    "Глобальный поиск по тегу ML:\n",
    "https://github.com/search?q=ML&type=repositories&s=stars&o=desc\n",
    "\n",
    "### 1.3 Описание предметной области\n",
    "\n",
    "GitHub — крупнейшая платформа для open-source разработки.\n",
    "Мы анализируем репозитории, отсортированные по популярности (метрика: количество звёзд, сортировка Most stars → по убыванию).\n",
    "Это позволяет выявлять факторы, связанные с востребованностью проектов в области AI/ML.\n",
    "\n",
    "---\n",
    "### 1.4 Требования и результат\n",
    "\n",
    "| Требование                             | Результат |\n",
    "|:--------------------------------------:|:-----------------------------:|\n",
    "| Количество наблюдений  > 1000          | 1685                         |\n",
    "| Начальное количество признаков >= 8-10 | 10                            |\n",
    "| Типы признаков                         | числовые (stars, forks, activity), категориальные (язык), даты (created/updated), бинарные (flags по стеку) |\n",
    "| Количество новых признаков >= 2        | 13                           |\n",
    "| Финальное количество признаков         | 23                            |\n",
    "| Визуализации >= 3                      | 8                             |\n",
    "| Итоговый формат                        | Jupyter/Colab notebook + .CSV |\n",
    "| [дополнительно] Репозиторий на github  | ссылка                        |\n",
    "\n",
    "\n",
    "### 1.5 Технологии проекта\n",
    "\n",
    "| Технология          | Для чего используем                               | Документация                                       |\n",
    "| :------------------ | :------------------------------------------------- | :------------------------------------------------- |\n",
    "| Python 3.12.12      | основная среда выполнения                          | https://docs.python.org/3/                         |\n",
    "| pandas              | работа с таблицами и предварительный анализ        | https://pandas.pydata.org/docs/                    |\n",
    "| NumPy               | численные операции и векторные расчёты             | https://numpy.org/doc/                             |\n",
    "| requests            | отправка HTTP-запросов к GitHub                    | https://requests.readthedocs.io/                   |\n",
    "| BeautifulSoup (bs4) | парсинг HTML-страниц результатов поиска            | https://beautiful-soup-4.readthedocs.io/en/latest/ |\n",
    "| seaborn             | быстрые статистические графики                     | https://seaborn.pydata.org/                        |\n",
    "| matplotlib          | базовая визуализация и настройка осей              | https://matplotlib.org/stable/                     |\n",
    "| ast                 | безопасное преобразование строк в списки/словари   | https://docs.python.org/3/library/ast.html         |\n",
    "| time (stdlib)       | паузы между запросами                              | https://docs.python.org/3/library/time.html        |\n",
    "| random (stdlib)     | рандомизация задержек при парсинге                 | https://docs.python.org/3/library/random.html      |\n",
    "| itertools (stdlib)  | вспомогательные итераторы при обходе данных        | https://docs.python.org/3/library/itertools.html   |\n",
    "| pathlib (stdlib)    | работа с путями и файлами                          | https://docs.python.org/3/library/pathlib.html     |\n",
    "\n",
    "\n",
    "### 1.6 Команда и распределение задач\n",
    "\n",
    "| Участник                         | Обязанности                         |\n",
    "|----------------------------------|-------------------------------------|\n",
    "| **Трофимов Матвей Владимирович** | Выбор первичных признаков, сбор и парсинг, первичные выводы     |\n",
    "| **Самунджян Дина Арменаковна**   | Очистка данных, пропуски, дубликаты, нормализация типов |\n",
    "| **Серенко Елена Валерьевна**     | Создание новых признаков, промежуточные выводы по сформированному датасету            |\n",
    "| **Пащенко Дмитрий Игоревич**     | Визуализации, промежуточные выводы по анализу и результаты   |\n",
    "\n",
    "### Структура ноутбука\n",
    "\n",
    "```markdown\n",
    "# 1. Введение\n",
    "## 1.1 Цель проекта\n",
    "## 1.2 Источник данных\n",
    "## 1.3 Описание предметной области\n",
    "## 1.4 Требования к датасету\n",
    "## 1.5 Технологии проекта\n",
    "## 1.6 Команда и распределение задач\n",
    "\n",
    "Трофимов Матвей Владимирович\n",
    "### 2. Сбор и парсинг данных\n",
    "2.1 Загрузка библиотек и модулей для проекта\n",
    "2.2 Функции для парсинга\n",
    "2.3 Сбор репозиториев (AI и ML) и объединение\n",
    "2.4 Набор полученных после парсинга признаков\n",
    "2.5 Парсинг. Промежуточные выводы\n",
    "\n",
    "Самунджян Дина Арменаковна\n",
    "### 3. Очистка данных\n",
    "3.1 Общий обзор датасета (shape, info, describe)\n",
    "3.2 Обработка пропусков и заполнение описаний\n",
    "3.3 Удаление дубликатов\n",
    "3.4 Преобразование Programming languages и флага languages_missing\n",
    "3.5 Очистка от выбросов по нескольким метрикам\n",
    "3.6 Очистка. Промежуточные выводы\n",
    "\n",
    "Серенко Елена Валерьевна\n",
    "### 4. Создание новых признаков\n",
    "4.1 Генерация временных меток Created и Last updated\n",
    "4.2 Подробное описание исходных столбцов\n",
    "4.3 Расчёт новых метрик вовлечённости и стека\n",
    "4.4 Блок разведочного анализа по активности сообществ\n",
    "4.5 Словарь новых признаков\n",
    "4.6 Обогащение. Промежуточные выводы\n",
    "\n",
    "Пащенко Дмитрий Игоревич\n",
    "### 5. Анализ данных и визуализация\n",
    "5.1 Описательная статистика\n",
    "5.2 Популярность языков\n",
    "5.3 Связи метрик и активность\n",
    "5.4 Итоговые выводы EDA\n",
    "\n",
    "\n",
    "### 6. Итоговые выводы всего исследования\n",
    "```\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 2. Сбор и парсинг данных"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\"\"\"\n",
    "Переключатель окружения для работы локально или в Google Colab.\n",
    "Устанавливайте USE_COLAB = True при запуске ноутбука в Colab или False, если хотите запускать локально.\n",
    "И в том и в другом случае вами нужно задать путь к рабочей директории! Спасибо:)\n",
    "\"\"\"\n",
    "from pathlib import Path\n",
    "\n",
    "USE_COLAB = True # ТУТ ПЕРЕКЛЮЧАТЬ!\n",
    "\n",
    "if USE_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive\")\n",
    "    DATA_DIR = Path(\"/content/drive/MyDrive/Colab Notebooks\") # Тут указана наша папка по умолчанию (/content/drive/MyDrive/Colab Notebooks), если у вас другая, можете изменить путь\n",
    "else:\n",
    "    DATA_DIR = Path(\"укажите адрес вашей локальной папки, где лежат датасеты и куда будут они сохраняться\")\n",
    "\n",
    "print(f\"Директория данных: {DATA_DIR.resolve()}\")\n",
    "\n",
    "# Далее в коде пути заменены на переменную DATA_DIR для удобства"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2.1 Загрузка библиотек и модулей для проекта"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import sys\n",
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "import random\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "\n",
    "NoneType = type(None)\n",
    "\n",
    "print(\"Python:\", sys.version.split()[0])\n",
    "print(\"requests:\", requests.__version__)\n",
    "print(\"pandas:\", pd.__version__)\n",
    "print(\"bs4:\", bs4.__version__)\n",
    "print(\"numpy:\", np.__version__)\n",
    "print(\"sns:\", sns.__version__)\n",
    "print(\"matplotlib:\", matplotlib.__version__)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2.2 Функции для парсинга\n",
    "\n",
    "**`repository_parsing(url)`**  \n",
    "Функция принимает URL репозитория GitHub и с помощью HTTP‑запроса и библиотеки BeautifulSoup извлекает его параметры: название, описание, языки программирования, количество форков, звезд, коммитов, пул‑реквестов, контрибьюторов и релизов. При ошибке парсинга или отсутствии нужных элементов возвращает `None`, чтобы проблемные репозитории можно было безопасно пропустить при массовом сборе данных.\n",
    "\n",
    "\n",
    "**`get_df_from_github(repositories_count)`**  \n",
    "Функция выполняет массовый сбор данных о репозиториях по запросу `AI` и `ML` в поиске GitHub, обходя до 100 страниц выдачи с паузами между запросами. Для каждой найденной ссылки вызывается `repository_parsing`, а собранные параметры добавляются в списки до достижения заданного количества репозиториев `repositories_count`, после чего формируется `pandas.DataFrame` с основными метриками и ссылками на репозитории.\n",
    "\n",
    "\n",
    "**`run_parsing()`**\n",
    "Функция инициализации пользователя для упрощения работы с ноутбуком. Будьте внимательны и читайте, что она у вас спрашивает.\n",
    "\n",
    "Смотрите подробности ниже в комментариях к коду."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def run_parsing():\n",
    "    response = input(\"\"\"Хотите запустить парсинг данных?\n",
    "\n",
    "    Введите 'yes' для запуска или 'no' для пропуска (мы учли любой регистр, не волнуйтесь):\n",
    "\n",
    "    В случае YES -- это может занять около 2 часов.\n",
    "\n",
    "    В случае NO -- у вас в директории должны лежать два файла github_repositories_AI.csv и github_repositories_ML.csv.\n",
    "\n",
    "    Если что, скачать их можно вот тут: https://drive.google.com/drive/folders/1g35lFmY8D1aj2Smb53L9YXjkrm6WfI69\n",
    "\n",
    "    Удачи :)\n",
    "\n",
    "    \"\"\").strip().lower()\n",
    "\n",
    "    if response == 'yes':\n",
    "        print(\"Запускаем парсинг данных... Это займет некоторое время.\")\n",
    "\n",
    "        # Запрашиваем количество репозиториев для парсинга у нашего преподавателя или его ассистентов :)\n",
    "        ai_repositories = input(\"Введите количество репозиториев для парсинга по тегу 'AI' (положительное целое число от 1 до 1000): \")\n",
    "        while not ai_repositories.isdigit() or int(ai_repositories) <= 0 or int(ai_repositories) > 1000:\n",
    "            ai_repositories = input(\"Пожалуйста, введите корректное количество репозиториев для AI (положительное целое число от 1 до 1000): \")\n",
    "\n",
    "        ai_repositories = int(ai_repositories)\n",
    "\n",
    "        ml_repositories = input(\"Введите количество репозиториев для парсинга по тегу 'ML'(положительное целое число от 1 до 1000): \")\n",
    "        while not ml_repositories.isdigit() or int(ml_repositories) <= 0:\n",
    "            ml_repositories = input(\"Пожалуйста, введите корректное количество репозиториев для ML (положительное целое число от 1 до 1000): \")\n",
    "\n",
    "        ml_repositories = int(ml_repositories)\n",
    "\n",
    "        df1 = get_df_from_github(ai_repositories, 'AI')\n",
    "        df2 = get_df_from_github(ml_repositories, 'ML')\n",
    "\n",
    "        return df1, df2\n",
    "\n",
    "    elif response == 'no':\n",
    "        print(\"Пропускаем парсинг. Загружаем датафреймы.\")\n",
    "        return None, None\n",
    "    else:\n",
    "        print(\"Некорректный ввод. Пожалуйста, введите 'yes' или 'no'.\")\n",
    "        return run_parsing()\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Получаем на вход ссылку на репозиторий в GitHub и парсим содержимое\n",
    "def repository_parsing(url):\n",
    "    try:\n",
    "        print(f'Парсим репозиторий: {url}')\n",
    "        site1 = requests.get(url)\n",
    "        site_text = site1.content.decode('utf-8')\n",
    "        soup = BeautifulSoup(site_text, 'html.parser')\n",
    "        split_url = url.split('/')\n",
    "        repository_name = split_url[-2] + ' ' + split_url[-1]\n",
    "\n",
    "        contributors = 0\n",
    "        releases = 0\n",
    "\n",
    "        # Считываем количество релизов со страницы репозитория.\n",
    "        for a in soup.find_all('a', class_='Link--primary no-underline Link'):\n",
    "            splitted = a.text.split()\n",
    "            if splitted[0] == 'Releases' and len(splitted) > 1:\n",
    "                releases = int(splitted[1].replace(',', '').replace(\"+\", \"\"))\n",
    "\n",
    "        # Считываем число контрибьюторов со страницы репозитория.\n",
    "        for a in soup.find_all('a', class_='Link--primary no-underline Link d-flex flex-items-center'):\n",
    "            splitted = a.text.split()\n",
    "            if splitted[0] == 'Contributors' and len(splitted) > 1:\n",
    "                contributors = int(splitted[1].replace(',', '').replace(\"+\", \"\"))\n",
    "\n",
    "        # Собираем перечень языков, указанных на странице репозитория.\n",
    "        languages = []\n",
    "        for span in soup.find_all('span', 'color-fg-default text-bold mr-1'):\n",
    "            languages.append(span.text)\n",
    "\n",
    "        # Инициализируем числовые метрики проекта.\n",
    "        pull_requests = 0\n",
    "        fork_amount = 0\n",
    "        stars_amount = 0\n",
    "        commits_amount = 0\n",
    "\n",
    "        # Берём описание репозитория, если оно присутствует.\n",
    "        if soup.find('p', class_='f4 my-3') != NoneType:\n",
    "            repository_description = soup.find('p', class_='f4 my-3').text.strip()\n",
    "\n",
    "        # Читаем счётчик открытых pull request.\n",
    "        if soup.find('span', class_='Counter', id='pull-requests-repo-tab-count') != NoneType:\n",
    "            pull_requests = int(soup.find('span', class_='Counter', id='pull-requests-repo-tab-count').get('title').replace(',', ''))\n",
    "\n",
    "        # Читаем количество форков.\n",
    "        if soup.find('span', id=\"repo-network-counter\") != NoneType:\n",
    "            fork_amount = int(soup.find('span', id=\"repo-network-counter\").get('title').replace(',', ''))\n",
    "\n",
    "        # Читаем количество звёзд.\n",
    "        if soup.find('span', id=\"repo-stars-counter-star\") != NoneType:\n",
    "            stars_amount = int(soup.find('span', id=\"repo-stars-counter-star\").get('title').replace(',', ''))\n",
    "\n",
    "        # Читаем количество коммитов.\n",
    "        if soup.find('span', class_='fgColor-default') != NoneType:\n",
    "            commits_amount = int(soup.find('span', class_='fgColor-default').text.split()[0].replace(',', ''))\n",
    "        # возвращаем список с параметрами репозитория\n",
    "        return [repository_name, repository_description, languages, fork_amount, stars_amount, commits_amount, pull_requests, contributors, releases, url]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при парсинге {url}: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_df_from_github(repositories_count, query_tag):\n",
    "    # Накопительные списки для формирования итогового DataFrame по репозиториям.\n",
    "    names = []\n",
    "    descriptions = []\n",
    "    languages = []\n",
    "    fork_amounts = []\n",
    "    stars_amounts = []\n",
    "    commits_amounts = []\n",
    "    pull_requests_amounts = []\n",
    "    contributors_amounts = []\n",
    "    releases_amounts = []\n",
    "    urls = []\n",
    "    source_path = 'https://github.com'\n",
    "    cur_repositories_count = 0\n",
    "    site1 = requests.get(f'https://github.com/search?q={query_tag}&type=repositories&s=stars&o=desc')\n",
    "    pages_count = repositories_count // 10 if repositories_count % 10 == 0 else repositories_count // 10 + 1\n",
    "    i = 0\n",
    "    while i < pages_count:\n",
    "        i += 1\n",
    "        if (i != 1):\n",
    "            time.sleep(10)  # Ставим паузу между страницами, чтобы не спамить GitHub (встретили проблему блокировок при высокочастотном парсинге).\n",
    "        print(f'Парсим репозитории с данного url: https://github.com/search?q={query_tag}&type=repositories&s=stars&o=desc&p={i}')\n",
    "        site1 = requests.get(f'https://github.com/search?q={query_tag}&type=repositories&s=stars&o=desc&p={i}')\n",
    "        print(f'Страница получена, с кодом ответа: {site1.status_code}')\n",
    "        while (site1.status_code != 200):  # Повторяем запрос, пока не получим корректный ответ.\n",
    "            time.sleep(10)\n",
    "            site1 = requests.get(f'https://github.com/search?q={query_tag}&type=repositories&s=stars&o=desc&p={i}')\n",
    "            print(f'Страница получена, с кодом ответа: {site1.status_code}')\n",
    "        site_text = site1.content.decode('utf-8')\n",
    "        soup = BeautifulSoup(site_text, 'html.parser')\n",
    "        for link in soup.find_all('a', class_='Link__StyledLink-sc-1syctfj-0 prc-Link-Link-85e08'):\n",
    "            if cur_repositories_count == repositories_count:  # Прекращаем сбор, как только набрали нужное число репозиториев.\n",
    "                break\n",
    "            repository_stat = repository_parsing(source_path + link.get('href'))\n",
    "            if repository_stat == None:  # Пропускаем репозиторий, если парсинг завершился ошибкой.\n",
    "                continue\n",
    "            cur_repositories_count += 1\n",
    "            # Сохраняем атрибуты репозитория для последующей сборки DataFrame.\n",
    "            names.append(repository_stat[0])\n",
    "            descriptions.append(repository_stat[1])\n",
    "            languages.append(repository_stat[2])\n",
    "            fork_amounts.append(repository_stat[3])\n",
    "            stars_amounts.append(repository_stat[4])\n",
    "            commits_amounts.append(repository_stat[5])\n",
    "            pull_requests_amounts.append(repository_stat[6])\n",
    "            contributors_amounts.append(repository_stat[7])\n",
    "            releases_amounts.append(repository_stat[8])\n",
    "            urls.append(repository_stat[9])\n",
    "    df = pd.DataFrame({\n",
    "        'Name': names,\n",
    "        'Description': descriptions,\n",
    "        'Programming languages': languages,\n",
    "        'Forks count': fork_amounts,\n",
    "        'Stars count': stars_amounts,\n",
    "        'Commits count': commits_amounts,\n",
    "        'Pull requests count': pull_requests_amounts,\n",
    "        'Contributors count': contributors_amounts,\n",
    "        'Releases counts': releases_amounts,\n",
    "        'Url': urls\n",
    "    })\n",
    "    return df\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df1, df2 = run_parsing()\n",
    "\n",
    "# Если парсинг был пропущен, можно загрузить данные из файлов.\n",
    "if df1 is None or df2 is None:\n",
    "    print(\"Загружаем датафреймы из CSV.\")\n",
    "    df1 = pd.read_csv(DATA_DIR / 'github_repositories_AI.csv')\n",
    "    df2 = pd.read_csv(DATA_DIR / 'github_repositories_ML.csv')"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# df1 = get_df_from_github(50, 'AI') # передаем кол-во репозиториев, которое хотим спарсить и тег, по которому производим поиск\n",
    "# df2 = get_df_from_github(50, 'ML') # передаем кол-во репозиториев, которое хотим спарсить и тег, по которому производим поиск"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df1.to_csv(DATA_DIR / 'github_repositories_AI.csv', index=False)\n",
    "df2.to_csv(DATA_DIR / 'github_repositories_ML.csv', index=False)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2.3 Сбор репозиториев (AI и ML) и объединение\n",
    "\n",
    "Совмещаем полученные датасеты в один."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df = pd.concat([df1, df2], ignore_index=True)\n",
    "df.to_csv(DATA_DIR / 'github_repositories.csv', index=False)\n",
    "df.shape"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Краткий словарь терминов\n",
    "- Stars (звёзды) — отметки пользователей, показывающие интерес к проекту; используют как метрику популярности.\n",
    "- Forks (форки) — копии репозитория, созданные для собственных изменений; отражают вовлечённость и желание доработать проект.\n",
    "- Commits (коммиты) — сохранения изменений в репозитории; показывают историю развития.\n",
    "- Pull requests (PR) — запросы на принятие изменений; число PR помогает оценить активность сообщества.\n",
    "- Contributors (контрибьюторы) — уникальные участники, отправлявшие изменения; важны для оценки «живости» проекта.\n",
    "- Releases (релизы) — официальные версии с зафиксированными изменениями; индикатор стабильности и зрелости продукта."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2.4 Набор полученных после парсинга признаков\n",
    "\n",
    "- Name — название репозитория.\n",
    "- Description — краткое текстовое описание репозитория.\n",
    "- Programming languages — перечень языков программирования.\n",
    "- Forks count — количество форков.\n",
    "- Stars count — количество звёзд.\n",
    "- Commits count — общее количество коммитов.\n",
    "- Pull requests count — количество открытых pull request.\n",
    "- Contributors count — количество контрибьюторов.\n",
    "- Releases counts — сколько релизов опубликовано.\n",
    "- Url — прямая ссылка на репозиторий."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2.5 Парсинг. Промежуточные выводы\n",
    "\n",
    "- Был получен датасет (по тегам AI и ML с 10 базовыми признаками), состоящий из 1953 наблюдений, отражающих основные метрики популярности и вовлеченности сообщества при работе с открытыми репозиториями в области исскуственного интеллекта и машинного обучения, а также технологические стеки, используемые в репозиториях.\n",
    "- Результат был сохранен в единый CSV-файл (github_repositories.csv) для удобства дальнейшего взаимодействия с полученными данными другими участниками команды.\n",
    "- При получении данных с помощью технологии веб-скрапинга была обнаружена проблема ограниченности данного метода. Некоторые данные, например, дата создания репозитория могут быть получены только с помощью API GitHub. С целью отражения знаний полученных на курсе и дальнейшего удобства визуализации данных,\n",
    "данный признак был сгенерирован исскуственно."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 3. Очистка данных"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Столбец Programming languages приведён к структурированному виду (список языков в каждой записи). Дополнительно создан флаг languages_missing для контроля записей, где языки отсутствуют."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.1 Общий обзор датасета"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df = pd.read_csv(DATA_DIR / 'github_repositories.csv')\n",
    "print(\"Shape:\", df.shape)\n",
    "df.head()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df.info()"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df.describe(include=\"all\").T"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.2 Обработка пропусков и заполнение описаний"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for col in [\"Name\", \"Description\", \"Url\"]:\n",
    "  if col in df.columns:\n",
    "    before_replacement = df[col].isna().sum()\n",
    "    df[col] = df[col].replace(r\"^\\s*$\", np.nan, regex=True)\n",
    "    after_replacement = df[col].isna().sum()\n",
    "\n",
    "    print(f'В колонке \"{col}\" заменено {after_replacement - before_replacement} значений на NaN.')"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Пустые значения нормализованы: в текстовых колонках пустые строки заменены на NaN, при этом структурные данные не затрагиваются."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if \"Description\" in df.columns:\n",
    "    before_fillna = df[\"Description\"].isna().sum()\n",
    "    df[\"Description\"] = df[\"Description\"].fillna(\"\")\n",
    "    after_fillna = df[\"Description\"].isna().sum()\n",
    "\n",
    "    print(f'В колонке \"Description\" заполнено {before_fillna} значений NaN пустыми строками.')\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Description: пропуски заполнены пустой строкой \"\" (чтобы анализ текста не ломался)."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "key_cols = [c for c in [\"Name\", \"Url\"] if c in df.columns]\n",
    "before = len(df)\n",
    "df = df.dropna(subset=key_cols)\n",
    "print(\"Удалено строк из-за пропусков в ключевых полях:\", before - len(df))"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(\"\\nПропуски по колонкам после очистки:\\n\", df.isna().sum().sort_values(ascending=False))"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.3 Удаление дубликатов"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if \"Url\" in df.columns:\n",
    "    df = df.drop_duplicates(subset=[\"Url\"], keep=\"first\")\n",
    "\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "print(\"Удалено дубликатов:\", before - len(df))"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Удалили строки-дубликаты (сначала по Url, затем полностью совпадающие), чтобы каждый репозиторий учитывался один раз и метрики не дублировались."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.5 Преобразование столбца Programming languages и флага languages_missing"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def parse_languages(x):\n",
    "    if pd.isna(x):\n",
    "        return []\n",
    "    if isinstance(x, list):\n",
    "        return [str(v).strip() for v in x if str(v).strip()]\n",
    "\n",
    "    s = str(x).strip()\n",
    "    if s in (\"\", \"[]\", \"nan\", \"None\"):\n",
    "        return []\n",
    "\n",
    "    if s.startswith(\"[\") and s.endswith(\"]\"):\n",
    "        inner = s[1:-1].strip()\n",
    "        if inner == \"\":\n",
    "            return []\n",
    "        parts = inner.split(\",\")\n",
    "        langs = []\n",
    "        for p in parts:\n",
    "            p = p.strip()\n",
    "            if (p.startswith(\"'\") and p.endswith(\"'\")) or (p.startswith('\\\"') and p.endswith('\\\"')):\n",
    "                p = p[1:-1]\n",
    "            p = p.strip()\n",
    "            if p:\n",
    "                langs.append(p)\n",
    "        return langs\n",
    "\n",
    "    return [s] if s else []\n",
    "\n",
    "# Применяем функцию и подсчитываем, сколько строк изменилось\n",
    "changes = 0\n",
    "for idx, value in df[\"Programming languages\"].items():\n",
    "    original_value = df.at[idx, \"Programming languages\"]\n",
    "    transformed_value = parse_languages(original_value)\n",
    "\n",
    "    if original_value != transformed_value:\n",
    "        changes += 1\n",
    "\n",
    "    df.at[idx, \"Programming languages\"] = transformed_value\n",
    "\n",
    "print(f\"Всего изменилось значений: {changes}\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.5 Очистка от выбросов по нескольким метрикам"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "k = 1.5\n",
    "need_at_least = 3  # удаляем, если выбросов >= 3 метрик\n",
    "\n",
    "releases_col = \"Releases count\" if \"Releases count\" in df.columns else \"Releases counts\"\n",
    "count_cols = [c for c in [\"Forks count\",\"Stars count\",\"Commits count\",\"Pull requests count\",\"Contributors count\",releases_col] if c in df.columns]\n",
    "\n",
    "flags = pd.DataFrame(index=df.index)\n",
    "\n",
    "for col in count_cols:\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    right = Q3 + k * IQR\n",
    "    flags[col] = df[col] > right\n",
    "\n",
    "to_drop = flags.sum(axis=1) >= need_at_least\n",
    "\n",
    "before = len(df)\n",
    "df = df[~to_drop].copy()\n",
    "print(\"Удалено строк (выбросы по >=3 метрикам):\", before - len(df))"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df.shape"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Для выбросов использовали множественный критерий: строка удалялась только если превышала порог IQR (Q3+1.5⋅IQR) сразу по нескольким (≥3) метрикам. Такой подход выбран как более мягкий, поскольку GitHub-показатели имеют выраженный “длинный хвост”, и одиночные экстремальные значения часто соответствуют реально популярным репозиториям, поэтому множественный критерий снижает риск удаления валидных наблюдений."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3.6 Очистка. Промежуточные выводы\n",
    "\n",
    "- **Пропуски (3.2):** пустые строки нормализованы в `Name/Description/Url` (0 значений заменено на `NaN`), `Description` заполнена без потерь (0 заполнений), ключевые поля без пропусков (`0` строк удалено), итоговые пропуски в столбцах — 0.\n",
    "- **Дубликаты (3.3):** удалений не потребовалось (`0` записей), структура датасета сохранена.\n",
    "- **Нормализация языков (3.5):** список `Programming languages` очищен и флаг `languages_missing` обновлён (изменено 20 значений).\n",
    "- **Выбросы (3.5):** удалена 1 запись, превышающая порог по ≥3 метрикам; размер датасета снизился с 1953 до 1952 строк.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 4. Создание новых признаков"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4.1 Генерация временных меток Created и Last updated\n",
    "\n",
    "Как упомянуто было ранее, при парсинге мы встретились с проблемой невозможности спарсить дату создания репозитория через веб-скрэйпинг, значит сгенерируем её синтетически.\n",
    "\n",
    "Created - дата создание репозитория  \n",
    "Last updated - время последнего изменения  \n",
    "Допустим, что репозитории созданы в период 2018–2024"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "n = len(df)\n",
    "\n",
    "start_date = pd.Timestamp(\"2018-01-01\")\n",
    "end_date   = pd.Timestamp(\"2024-12-31\")\n",
    "\n",
    "created_random = start_date + pd.to_timedelta(np.random.randint(0, (end_date - start_date).days + 1, size=n),unit=\"D\")\n",
    "df[\"Created\"] = created_random\n",
    "\n",
    "delta_days = np.random.randint(0, 365 * 3, size=n)\n",
    "df[\"Last updated\"] = df[\"Created\"] + pd.to_timedelta(delta_days, unit=\"D\")\n",
    "df[\"Last updated\"] = df[\"Last updated\"].clip(upper=end_date)\n",
    "df.head(2)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4.2 Подробное описание исходных столбцов\n",
    "\n",
    "- **Name** - Название репозитория на GitHub. Это короткое имя, которое отображается в URL и в интерфейсе GitHub.  \n",
    "- **Description** - Текстовое описание репозитория, которое автор указал в настройках. В неи обычно пишут, что делает проект, для кого он, какие у него основные фичи.  \n",
    "- **Programming languages** - Список языков, которые GitHub определил в этом репозитории по коду.  \n",
    "- **Forks count** - Количество форков репозитория. Форк — это копия проекта, которую кто-то сделал к себе в аккаунт, чтобы доработать/изменять код. Чем больше форков, тем чаще проект используют как основу для своих решений, фреймворков, экспериментов.  \n",
    "- **Stars count** - Количество «звёзд» — лайков, которые пользователи поставили репозиторию на GitHub. Это простой индикатор популярности / интереса к проекту.  \n",
    "- **Commits count** - Общее количество коммитов (фиксаций изменений) в репозитории. Это показывает, сколько раз код изменяли за всё время существования проекта.  \n",
    "- **Pull requests count** - Общее количество pull requests (PR) в репозитории. PR — это запросы на вливание изменений, часто от внешних участников. Это важный индикатор того, насколько активно комьюнити участвует в развитии проекта: много PR → много внешних вкладчиков/изменений.  \n",
    "- **Contributors count** - Количество контрибьюторов — людей, которые внесли изменения в код (хотя бы один коммит). Это размер команды/сообщества разработчиков. Репозиторий с 2 контрибьюторами и репозиторий с 300 контрибьюторами — очень разные по масштабу проекты.  \n",
    "- **Releases count** - Сколько релизов (официальных версий) опубликовано в репозитории через GitHub Releases. Это говорит о том, насколько «продуктово» ведётся проект: есть ли формальные версии, changelog, релизные циклы. У библиотек/фреймворков обычно релизов много, у демо/песочниц — мало или вообще нет.  \n",
    "- **Url** - Прямая ссылка на репозиторий на GitHub."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4.3 Расчёт новых метрик вовлечённости и стека"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- **Stars per contributor** - отражает насколько много звёзд приходится на одного участника, позволяет сравнить маленькие и большие команды.  \n",
    "\n",
    "Гипотеза: у маленьких, но популярных библиотек значение может быть очень высоким, а у огромных инфраструктурных проектов — ниже, при том что звёзд много. Попробовать сравнить медианы Stars per contributor для разных типов проектов: библиотека/платформа, Python vs. не-Python и т.д."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df[\"Stars per contributor\"] = round(df[\"Stars count\"] / (df[\"Contributors count\"] + 1), 1)\n",
    "df.head(1)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- **Commits per contributor** - интенсивность вклада одного участника: среднее число коммитов на одного контрибьютора.\n",
    "Можно интерпретировать как «насколько активно работает каждый участник».\n",
    "\n",
    "Гипотеза: у популярных репозиториев выше Commits per contributor, чем у непопулярных, посмотреть, отличается ли это между «демо»-репозиториями и серьёзными фреймворками."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df[\"Commits per contributor\"] = round(df[\"Commits count\"] / (df[\"Contributors count\"] + 1), 1)\n",
    "df.head(1)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- **Community openness** - доля изменений через PR (индекс открытости). Если Community openness близко к 0, значит почти всё делают прямыми коммитами, команда закрытая, а если ближе к 1, это значит что основная жизнь идёт через PR, проект открыт к внешним вкладчикам.\n",
    "\n",
    "Гипотеза: популярные проекты (по звёздам) чаще имеют высокий Community openness, а образовательные демо-репозитории, наоборот, почти не принимают PR."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df[\"Total changes\"] = df[\"Commits count\"] + df[\"Pull requests count\"]\n",
    "df[\"Community openness\"] = df[\"Pull requests count\"] / (df[\"Total changes\"] + 1)\n",
    "df.head(1)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- **Num languages** - стек технологий и «сложность» проекта, то есть, чем больше языков, тем более «широкий» стек (frontend+backend+infra и т.д.).<br>\n",
    "\n",
    "Гипотеза: проекты с большим числом использованных языков, особенно когда это включает такие популярные языки, как Python и TypeScript, имеют более широкий стек технологий, что может указывать на большую гибкость и сложность проекта. Большая часть топовых репозиториев, как показывает статистика, часто использует такие языки, как Python для backend-разработки и TypeScript для frontend."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df[\"Num languages\"] = df[\"Programming languages\"].apply(len)\n",
    "df.head(1)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "backend_set  = {\"Python\", \"Java\", \"Go\", \"C++\", \"Rust\", \"C#\", \"Ruby\"}\n",
    "frontend_set = {\"JavaScript\", \"TypeScript\", \"HTML\", \"CSS\"}\n",
    "infra_set    = {\"Dockerfile\", \"Shell\"}\n",
    "data_set     = {\"SQL\", \"PLpgSQL\", \"R\"}\n",
    "\n",
    "def classify_stack(langs):\n",
    "    langs = set(langs)\n",
    "    has_backend  = len(langs & backend_set) > 0\n",
    "    has_frontend = len(langs & frontend_set) > 0\n",
    "    has_infra    = len(langs & infra_set) > 0\n",
    "    has_data     = len(langs & data_set) > 0\n",
    "    return pd.Series({\n",
    "        \"Has backend\": has_backend,\n",
    "        \"Has frontend\": has_frontend,\n",
    "        \"Has infra\": has_infra,\n",
    "        \"Has data\": has_data,\n",
    "        \"Stack breadth\": sum([has_backend, has_frontend, has_infra, has_data]),\n",
    "    })\n",
    "\n",
    "stack_features = df[\"Programming languages\"].apply(classify_stack)\n",
    "df = pd.concat([df, stack_features], axis=1)\n",
    "\n",
    "df.head(1)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- **Hype score** - Hype-индекс» по модным словам (LLM, agents, RAG…) отражает, чем занимается проект.\n",
    "\n",
    "Гипотеза: проекты, содержащие больше таких «модных» слов, как LLM, agents, RAG, чаще всего являются новыми, актуальными и активно развивающимися, что может указывать на их высокую востребованность и современность. Гипотеза основывается на том, что проекты, ориентированные на новейшие технологии, привлекают больше внимания, пользователей и контрибьюторов."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "buzzwords = [\"llm\", \"agent\", \"rag\", \"workflow\", \"chatbot\", \"no-code\", \"low-code\"]\n",
    "\n",
    "def hype_score(s):\n",
    "    return sum(word in s for word in buzzwords)\n",
    "\n",
    "df[\"Hype score\"] = df[\"Description\"].apply(hype_score)\n",
    "df.head(1)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df.shape"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4.4 Блок разведочного анализа по активности сообществ\n",
    "Здесь фиксируем цель всех следующих шагов: понять, какие метрики (звезды, форки, коммиты) сильнее всего характеризуют крупные open-source проекты и как они взаимосвязаны между собой.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Словарь новых признаков после обогащения:\n",
    "\n",
    "- **Created** — сгенерированная дата создания репозитория, нужна для условного временного анализа.\n",
    "- **Last updated** — дата последнего изменения (обрезана по 2024-12-31), позволяет оценить свежесть проектов.\n",
    "- **Stars per contributor** — среднее число звёзд на участника, сравнивает популярность с учётом размера команды.\n",
    "- **Commits per contributor** — среднее число коммитов на участника, отражает типичную нагрузку на одного разработчика.\n",
    "- **Total changes** — сумма коммитов и PR, базовая метрика общего объёма изменений.\n",
    "- **Community openness** — доля изменений через pull request, показывает, насколько проект открыт к внешнему вкладу.\n",
    "- **Num languages** — количество языков в стеке, грубая оценка технологической широты.\n",
    "- **Has backend / Has frontend / Has infra / Has data** — булевы флаги наличия соответствующих компонентов в стеке.\n",
    "- **Stack breadth** — суммарное число найденных компонент стека, быстрый индикатор полноты (backend+frontend+infra+data).\n",
    "- **Hype score** — счётчик модных слов из описания (LLM, agents, RAG и т.д.), помогает выделить трендовые темы.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4.6 Обогащение. Промежуточные выводы\n",
    "\n",
    "* Добавили временные признаки (created/updated) и сгруппировали активность по дням, чтобы сравнивать репозитории по темпу роста.\n",
    "* Посчитали итоговые метрики вовлечённости (отношения stars/forks к времени жизни, PR/issue-рейты, доля контрибьюторов), чтобы видеть, насколько проект живой.\n",
    "* Описали словарь новых признаков и флаги по стеку (например, наличие Python/JS/Java), чтобы упростить фильтрацию и сравнение языков.\n",
    "* После обогащения финальный датасет вырос до 23 столбцов и позволяет строить более содержательные графики по активности сообществ.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 5. Анализ данных и визуализация\n",
    "\n",
    "## 5.1 Описательная статистика\n",
    "\n",
    "#### Исследуем базовую статистику по ключевым числовым признакам, чтобы оценить порядок величин и разброс активности в выборке.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df_without_cols = df.drop(['Created', 'Last updated'], axis=1) # исключаем признаки типа \"дата\"\n",
    "df_without_cols.describe().round(2).T"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Выводы по описательной статистике.** Распределения звёзд и форков сильно скошены вправо: у большинства проектов десятки/сотни оценок, но есть небольшой слой сверхпопулярных репозиториев. PR и релизы часто нулевые, поэтому активность оформлена не у всех проектов. Число контрибьюторов колеблется от одиночных разработчиков до крупных команд, что говорит о разном уровне зрелости проектов.\n"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "#### Построим матрицу корреляций по основным числовым признакам популярности и активности репозиториев.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "corr_cols = [\n",
    "    'Stars count', 'Forks count', 'Commits count', 'Pull requests count',\n",
    "    'Contributors count', 'Stars per contributor', 'Commits per contributor',\n",
    "    'Community openness', 'Num languages'\n",
    "]\n",
    "corr_df = df[corr_cols].corr().round(2)\n",
    "corr_df"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(corr_df, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title('Корреляция ключевых числовых метрик')\n",
    "plt.show()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Вывод по корреляции.** Самая сильная связь наблюдается между числами звёзд и форков, чуть слабее — между звёздами и количеством контрибьюторов. Почти отсутствует линейная связь между числом языков и активностными метриками, что видно по низким корреляциям в соответствующих строках/столбцах."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5.2 Популярность языков и стэков\n",
    "\n",
    "#### Посмотрим, какие языки чаще всего встречаются как основной язык репозитория.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "lang_series = df['Programming languages'].apply(\n",
    "    lambda x: x if isinstance(x, list) else ast.literal_eval(x) if isinstance(x, str) else []\n",
    ")\n",
    "lang_counts = lang_series.explode().value_counts()\n",
    "language_percent = (lang_counts / lang_counts.sum()) * 100\n",
    "\n",
    "# Выделяем языки с долей >= 3%, остальные объединяем в \"Other\"\n",
    "top_languages = language_percent[language_percent >= 3]\n",
    "other_percent = 100 - top_languages.sum()\n",
    "languages_for_pie = pd.concat([top_languages, pd.Series({'Other': other_percent})])\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "colors = sns.color_palette('Set3', n_colors=len(languages_for_pie))\n",
    "languages_for_pie.plot(kind='pie', autopct='%1.1f%%', startangle=90, colors=colors, legend=False)\n",
    "plt.title('Распределение языков в проектах')\n",
    "plt.ylabel('')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Средние звёзды по стэку\n",
    "\n",
    "**Цель подсчёта средних звёзд по стэку.** Хотим понять, дают ли разные комбинации языков заметное преимущество по популярности и выделяются ли гибридные стэки среди лидеров.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "stacked_langs = lang_series.apply(lambda x: \", \".join(x) if isinstance(x, list) else str(x))\n",
    "mean_stars_by_stack = (\n",
    "    df.assign(Stack=stacked_langs)\n",
    "      .groupby('Stack')['Stars count']\n",
    "      .mean()\n",
    "      .sort_values(ascending=False)\n",
    "      .head(10)\n",
    ")\n",
    "mean_stars_by_stack\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Выводы по языкам и популярности.** Python остаётся главным языком выборки; далее идут JavaScript/TypeScript и Shell. В топовых по звёздам стэках чаще встречается сочетание Python с фронтенд-языками, то есть гибридные проекты собирают больше внимания, чем одиночные инфраструктурные репозитории.\n"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### План визуализаций по языкам\n",
    "Мы исследуем популярность технологий и связь метрик активности с популярностью репозиториев: топовые языки, распределения звёзд/форков по стеку, форма зависимости Stars-Forks, корреляции активности (Commits, PRs, Contributors, Stars) и влияние количества контрибьюторов/PR на звёзды.\n",
    "Отдельно смотрим на `hype score`, чтобы сопоставить модные темы с популярностью.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Топ-10 языков\n",
    "Визуализируем топ-10 основных языков, чтобы оценить технологическую популярность в выборке.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# Преобразуем столбец 'Created' в datetime и разворачиваем списки языков\n",
    "created_dt = pd.to_datetime(df['Created'], errors='coerce')\n",
    "timeline = pd.DataFrame({\n",
    "    'Created': created_dt,\n",
    "    'Programming languages': lang_series\n",
    "}).explode('Programming languages').dropna(subset=['Programming languages'])\n",
    "\n",
    "# Отбираем топ-10 языков по количеству репозиториев\n",
    "top_10_languages = timeline['Programming languages'].value_counts().head(10).index\n",
    "trend_data = (\n",
    "    timeline[timeline['Programming languages'].isin(top_10_languages)]\n",
    "    .assign(**{'Year-Month': lambda d: d['Created'].dt.to_period('M').dt.to_timestamp()})\n",
    "    .groupby(['Year-Month', 'Programming languages'])\n",
    "    .size()\n",
    "    .reset_index(name='Repositories')\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.lineplot(\n",
    "    data=trend_data,\n",
    "    x='Year-Month',\n",
    "    y='Repositories',\n",
    "    hue='Programming languages',\n",
    "    marker='o',\n",
    "    dashes=False,\n",
    "    palette='tab10'\n",
    ")\n",
    "plt.title('Прирост создания репозиториев по топ-10 языкам', fontsize=16)\n",
    "plt.xlabel('Год-Месяц', fontsize=12)\n",
    "plt.ylabel('Количество репозиториев', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right', fontsize=10)\n",
    "plt.legend(title='Языки', loc='upper left', bbox_to_anchor=(1, 1))\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "**Вывод по топ-10 языкам.** Python и JavaScript уверенно лидируют по числу репозиториев; TypeScript, Shell и Jupyter Notebook формируют следующий кластер, отражая популярность веб- и data-стеков. Лидеры сохраняют отрыв на всём горизонте наблюдения.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Распределения звёзд/форков по топ-5 языкам\n",
    "Сравниваем разброс популярности по стеку через boxplot/violin.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "sns.violinplot(\n",
    "    data=lang_long,\n",
    "    x='Stars count',\n",
    "    y='Programming languages',\n",
    "    cut=0,\n",
    "    ax=axes[0]\n",
    ")\n",
    "axes[0].set_title('Распределение звёзд по языкам')\n",
    "axes[0].set_xlabel('Количество звёзд')\n",
    "axes[0].set_ylabel('Язык')\n",
    "axes[0].set_xlim(0, stars_q99)\n",
    "\n",
    "sns.boxplot(\n",
    "    data=lang_long,\n",
    "    x='Forks count',\n",
    "    y='Programming languages',\n",
    "    ax=axes[1]\n",
    ")\n",
    "axes[1].set_title('Распределение форков по языкам')\n",
    "axes[1].set_xlabel('Количество форков')\n",
    "axes[1].set_ylabel('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Вывод по распределениям.** У Python самый широкий разброс звёзд и форков: встречаются как небольшие, так и очень популярные проекты. JavaScript и TypeScript сгруппированы ближе к средним значениям, а Shell и Jupyter Notebook дают более компактные распределения, что отражает их нишевые сценарии.\n"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5.3 Связи метрик и активность\n",
    "\n",
    "#### Scatter Stars–Forks в лог-шкале\n",
    "Проверяем форму связи между звёздами и форками в логарифмическом масштабе.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "scatter_df = df[(df['Stars count'] > 0) & (df['Forks count'] > 0)]\n",
    "low = scatter_df[['Stars count', 'Forks count']].quantile(0.01)\n",
    "high = scatter_df[['Stars count', 'Forks count']].quantile(0.99)\n",
    "scatter_df = scatter_df[\n",
    "    scatter_df['Stars count'].between(low['Stars count'], high['Stars count']) &\n",
    "    scatter_df['Forks count'].between(low['Forks count'], high['Forks count'])\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(7, 6))\n",
    "hb = plt.hexbin(\n",
    "    scatter_df['Forks count'],\n",
    "    scatter_df['Stars count'],\n",
    "    gridsize=40,\n",
    "    cmap='magma',\n",
    "    norm=matplotlib.colors.LogNorm(),\n",
    "    mincnt=1\n",
    ")\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Forks count (log)')\n",
    "plt.ylabel('Stars count (log)')\n",
    "plt.title('Зависимость Stars vs. Forks\\nлог-шкала и плотность точек')\n",
    "cb = plt.colorbar(hb)\n",
    "cb.set_label('log(количество репозиториев в бине)')\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Вывод по Stars–Forks.** В логарифмической шкале точки выстраиваются почти в линию: чем больше форков, тем больше звёзд. Сверхпопулярные репозитории формируют редкие выбросы, а основное облако показывает типичную степенную зависимость.\n"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Коррелограмма ключевых метрик\n",
    "Ищем взаимосвязи между Stars, Forks, Commits, PR и Contributors.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "activity_cols = ['Stars count', 'Forks count', 'Commits count', 'Pull requests count', 'Contributors count']\n",
    "sns.set_theme(style='whitegrid')\n",
    "g = sns.pairplot(\n",
    "    df[activity_cols],\n",
    "    corner=True,\n",
    "    diag_kind='hist',\n",
    "    plot_kws={'alpha': 0.3, 's': 20}\n",
    ")\n",
    "g.fig.suptitle('Связи метрик вовлечённости и популярности', y=1.02)\n",
    "plt.show()\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Вывод по коррелограмме.** Самая заметная связь — между звёздами и форками. Больше коммитов и PR идёт рука об руку с ростом числа контрибьюторов, тогда как количество языков почти не влияет на популярность.\n"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Трендлайны влияния контрибьюторов и PR на звёзды\n",
    "Сравниваем наклоны линий зависимости Stars от числа контрибьюторов и PR.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5), sharey=True)\n",
    "for ax, (xcol, title) in zip(axes, [('Contributors count', 'влияние контрибьюторов'), ('Pull requests count', 'влияние PR')]):\n",
    "    reg_df = df[(df['Stars count'] > 0) & (df[xcol] > 0)]\n",
    "    reg_df = reg_df[\n",
    "        (reg_df['Stars count'] < reg_df['Stars count'].quantile(0.995)) &\n",
    "        (reg_df[xcol] < reg_df[xcol].quantile(0.995))\n",
    "    ]\n",
    "    sns.regplot(\n",
    "        data=reg_df,\n",
    "        x=xcol,\n",
    "        y='Stars count',\n",
    "        scatter_kws={'alpha': 0.25, 's': 18},\n",
    "        line_kws={'color': 'darkred'},\n",
    "        ci=None,\n",
    "        ax=ax\n",
    "    )\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_yscale('log')\n",
    "    ax.set_title(f\"Trendline: Stars ~ {title}\\nдля количественной оценки влияния\")\n",
    "    ax.set_xlabel(f\"{xcol} (log)\")\n",
    "    ax.set_ylabel('Stars count (log)')\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Вывод по трендовым линиям.** При увеличении числа контрибьюторов звёзды растут быстрее, чем при росте количества PR: дополнительный участник даёт больший вклад, а объём PR работает скорее как поддерживающий фактор.\n"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5.4 Итоговые выводы EDA\n",
    "- **Главные языки.** Большая часть репозиториев построена на Python; далее идут JavaScript/TypeScript и другие вспомогательные языки.\n",
    "- **Распределения популярности.** У звёзд и форков длинные правые хвосты: большинство проектов набирают умеренные показатели, но есть редкие лидеры с десятками тысяч оценок.\n",
    "- **Связи метрик.** Звёзды растут вместе с форками; активность (коммиты, PR) и число контрибьюторов движутся в одну сторону, но эффект слабее, чем у пары Stars–Forks.\n",
    "- **Роль сообщества.** Больше участников даёт более заметный прирост звёзд, чем просто увеличение числа PR, поэтому вовлечение людей важнее, чем объём изменений.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 6. Итоговые выводы исследования (общий шаблон)\n",
    "\n",
    ">Ребята, замените подсказки своими итогами.\n",
    "\n",
    "**Предлагаемый формат заполнения:**\n",
    "- Матвей: кратко о составе и сборе датасета (объём, источники, качество парсинга).\n",
    "- Дина: ключевые выводы об очистке и качестве данных (пропуски, дубликаты, выбросы).\n",
    "- Лена: влияние созданных признаков и как они улучшают анализ активности/стека.\n",
    "- Дима: основные инсайты EDA и визуализаций о популярности и взаимосвязях метрик.\n",
    "- Итог: 3–4 буллета с общими выводами и 1 пункт с ограничениями/идеями для будущей работы.\n",
    "\n"
   ]
  }
 ]
}
